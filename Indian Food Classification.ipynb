{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras\n",
    "import pydot\n",
    "import multiprocessing\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model,load_model\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D,concatenate, AveragePooling2D, Concatenate,Input,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from PIL import Image , ImageOps\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import random , os , math \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import densenet\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from tensorflow.python.framework import graph_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1(x):\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    \n",
    "    x2 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "\n",
    "    x2 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x2)\n",
    "    x2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "\n",
    "    x3 = AveragePooling2D(pool_size=(2, 2), strides=None, data_format=None)(x)\n",
    "    \n",
    "    x3 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' ,activation='relu')(x3)\n",
    "    x3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x3)\n",
    "    x3 = Activation('relu')(x3)\n",
    "    \n",
    "    x4 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x4 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x4)\n",
    "    x4 = Activation('relu')(x4)\n",
    "    \n",
    "    final = concatenate([x1,x2,x3,x4],axis=-1)\n",
    "    \n",
    "    return final\n",
    "\n",
    "\n",
    "def block2(x):\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x2 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "\n",
    "    x2 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x2)\n",
    "    x2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "\n",
    "    x2 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x2)\n",
    "    x2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "\n",
    "    x3 = AveragePooling2D(pool_size=(2, 2), strides=None, data_format=None)(x)\n",
    "    \n",
    "    x3 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' ,activation='relu')(x3)\n",
    "    x3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x3)\n",
    "    x3 = Activation('relu')(x3)\n",
    "    \n",
    "    x4 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x4 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x4)\n",
    "    x4 = Activation('relu')(x4)\n",
    "    \n",
    "    final = concatenate([x1,x2,x3,x4],axis=-1)\n",
    "    \n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def block3(x):\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    x1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "    # branching happen here\n",
    "    \n",
    "    #sub branch one start\n",
    "    x1_1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1_1)\n",
    "    x1_1 = Activation('relu')(x1_1)\n",
    "    #sub branch one end\n",
    "    \n",
    "    #sub branch two start\n",
    "    x1_2 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x1)\n",
    "    x1_2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x1_2)\n",
    "    x1_2 = Activation('relu')(x1_2)\n",
    "    #sub branch one end\n",
    "    \n",
    "    branchoneConcat = concatenate([x1_1,x1_2],axis=-1)\n",
    "    \n",
    "    \n",
    "    # branch two start here\n",
    "    x2 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "\n",
    "        #sub branch two start\n",
    "    x2_1 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x2)\n",
    "    x2_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2_1)\n",
    "    x2_1 = Activation('relu')(x2_1)\n",
    "\n",
    "        #sub branch two start\n",
    "    x2_2 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' , activation='relu')(x2)\n",
    "    x2_2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x2_2)\n",
    "    x2_2 = Activation('relu')(x2_2)\n",
    "\n",
    "    branchtwoConcat = concatenate([x2_1,x2_2],axis=-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x3 = AveragePooling2D(pool_size=(2, 2), strides=None, data_format=None)(x)\n",
    "    \n",
    "    x3 = Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same' ,activation='relu')(x3)\n",
    "    x3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x3)\n",
    "    x3 = Activation('relu')(x3)\n",
    "    \n",
    "    x4 = Conv2D(filters=32, kernel_size=(3,3),strides=2,padding='same' , activation='relu')(x)\n",
    "    x4 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x4)\n",
    "    x4 = Activation('relu')(x4)\n",
    "    \n",
    "    \n",
    "    final = concatenate([branchoneConcat,branchtwoConcat,x3,x4],axis=-1)\n",
    "    \n",
    "    return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def createModel8():\n",
    "    ModelInput = Input(shape=(224,224,3), name = 'input_0')\n",
    "    x = Conv2D(filters=32, kernel_size=(3,3),strides=2, activation='relu')(ModelInput)\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3,3),strides=1, activation='relu')(x)\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3),strides=1, activation='relu')(x)\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=2)(x)\n",
    "\n",
    "    x = Conv2D(filters=80, kernel_size=(3,3),strides=1, activation='relu')(x)\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters=192, kernel_size=(3,3),strides=1, activation='relu')(x)\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=2)(x)\n",
    "    \n",
    "    block_1_output = block1(x)\n",
    "\n",
    "    block_2_output = block2(block_1_output)\n",
    "\n",
    "    block_3_output = block3(block_2_output)\n",
    "    \n",
    "    GAP = GlobalAveragePooling2D()(block_3_output)\n",
    "    \n",
    "    output = Dense(48,activation='softmax')(GAP)\n",
    "    \n",
    "    model = Model(inputs=ModelInput, outputs=output)\n",
    "    return model\n",
    "\n",
    "#model = createModel8()\n",
    "#SVG(model_to_dot(model).create(prog='dot',format='svg'))\n",
    "#plot_model(model, to_file='ourmodel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7e377ba147a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_7/Checkpoints/cp-0014.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "\n",
    "model.load_weights('training_7/Checkpoints/cp-0014.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33599 images belonging to 48 classes.\n",
      "Found 14400 images belonging to 48 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        #horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './DATASET/Training',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=5,\n",
    "        #save_to_dir='./Augmented/Training',\n",
    "        save_prefix='aug', \n",
    "        save_format='jpg',\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './DATASET/Validation',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=5,\n",
    "        #save_to_dir='./Augmented/Validation',\n",
    "        save_prefix='aug', \n",
    "        save_format='jpg',\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "# i = 0\n",
    "# for batch in train_generator:\n",
    "#     i += 1\n",
    "#     if i > 20:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_0 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 111, 111, 32) 896         input_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 111, 111, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 111, 111, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 109, 109, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 109, 109, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 109, 109, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 107, 107, 64) 18496       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 107, 107, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 107, 107, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 53, 53, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 51, 51, 80)   46160       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 51, 51, 80)   320         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 51, 51, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 49, 49, 192)  138432      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 49, 49, 192)  768         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 49, 49, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 24, 24, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 12, 12, 32)   55328       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 12, 12, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 12, 12, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 12, 12, 32)   9248        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 12, 12, 32)   55328       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 12, 12, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 12, 12, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 12, 12, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 12, 12, 32)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 12, 12, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 12, 12, 32)   9248        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 12, 12, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 12, 12, 32)   55328       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 12, 12, 32)   55328       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 12, 12, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 12, 12, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 12, 12, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 12, 12, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 12, 12, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 12, 12, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 12, 12, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 12, 12, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12, 12, 128)  0           activation_8[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 6, 6, 32)     36896       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 6, 6, 32)     128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 6, 6, 32)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 6, 6, 32)     9248        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 6, 6, 32)     128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 6, 6, 32)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 6, 6, 32)     9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 6, 6, 32)     36896       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 6, 6, 32)     128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 6, 6, 32)     128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 6, 6, 32)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 6, 6, 32)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 6, 6, 32)     9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 6, 6, 32)     9248        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 6, 6, 32)     128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 6, 6, 32)     128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 6, 6, 32)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 6, 6, 32)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 6, 6, 128)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 6, 6, 32)     9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 6, 6, 32)     9248        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 6, 6, 32)     36896       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 6, 6, 32)     36896       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 6, 6, 32)     128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 6, 6, 32)     128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 6, 6, 32)     128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 6, 6, 32)     128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 6, 6, 32)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 6, 6, 32)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 6, 6, 32)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 6, 6, 32)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 6, 6, 128)    0           activation_17[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 3, 3, 32)     36896       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 3, 3, 32)     128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 3, 3, 32)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 3, 3, 32)     9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 3, 3, 32)     36896       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 3, 3, 32)     128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 3, 3, 32)     128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 3, 3, 32)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 3, 3, 32)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 3, 3, 32)     9248        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 3, 3, 32)     9248        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 3, 3, 32)     9248        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 3, 3, 32)     9248        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 3, 3, 128)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 3, 3, 32)     128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 3, 3, 32)     128         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 3, 3, 32)     128         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 3, 3, 32)     128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 3, 3, 32)     36896       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 3, 3, 32)     36896       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 3, 3, 32)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 3, 3, 32)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 3, 3, 32)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 3, 3, 32)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 3, 3, 32)     128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 3, 3, 32)     128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 3, 3, 64)     0           activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3, 3, 64)     0           activation_28[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 3, 3, 32)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 3, 3, 32)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 3, 3, 192)    0           concatenate_3[0][0]              \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 192)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48)           9264        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 873,376\n",
      "Trainable params: 870,912\n",
      "Non-trainable params: 2,464\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "2000/2000 [==============================] - 1524s 762ms/step - loss: 3.2957 - acc: 0.1463 - val_loss: 3.7459 - val_acc: 0.1438\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.74589, saving model to training_7/Checkpoints/cp-0001.ckpt\n",
      "Epoch 2/64\n",
      "2000/2000 [==============================] - 1520s 760ms/step - loss: 3.2474 - acc: 0.1617 - val_loss: 3.3001 - val_acc: 0.2054\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.74589 to 3.30005, saving model to training_7/Checkpoints/cp-0002.ckpt\n",
      "Epoch 3/64\n",
      "2000/2000 [==============================] - 1514s 757ms/step - loss: 3.2068 - acc: 0.1696 - val_loss: 3.7795 - val_acc: 0.1590\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.30005\n",
      "Epoch 4/64\n",
      " 869/2000 [============>.................] - ETA: 10:30 - loss: 3.1499 - acc: 0.1798"
     ]
    }
   ],
   "source": [
    "# config\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "def do_training():\n",
    "    epochs = 64\n",
    "\n",
    "    checkpoint_path = \"training_7/Checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create checkpoint callback\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                     #save_weights_only=True,\n",
    "                                                     save_best_only = True,\n",
    "                                                     verbose=1,period=1)\n",
    "\n",
    "\n",
    "\n",
    "    model = createModel8()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])\n",
    "    model.load_weights(\"training_7/Checkpoints/cp-0013.ckpt\")\n",
    "\n",
    "    image_dim = (224, 224, 3)\n",
    "\n",
    "    #model = densenet.DenseNet(classes=65, input_shape=image_dim, depth=19, growth_rate=12,bottleneck=True, reduction=0.5)\n",
    "    print(model.summary())\n",
    "\n",
    "    \n",
    "    result = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=2000,\n",
    "            callbacks=[cp_callback],\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=1000\n",
    "    )\n",
    "\n",
    "result = multiprocessing.Process(target=do_training())\n",
    "\n",
    "result.start()\n",
    "result.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/undercover/anaconda3/envs/indianFood/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2074s 2s/step - loss: 3.1210 - acc: 0.1816 - val_loss: 3.9387 - val_acc: 0.1660\n",
      "\n",
      "Epoch 00001: saving model to training_5/Checkpoints_2/\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'training_5/Checkpoints_2/', errno = 21, error message = 'Is a directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5691b065e89f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    597\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch %05d: saving model to %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproceed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indianFood/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'training_5/Checkpoints_2/', errno = 21, error message = 'Is a directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "checkpoint_path= \"training_5/Checkpoints/cp-0004.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,period=1)\n",
    "epochs = 10\n",
    "if checkpoint_path is not None:\n",
    "    # Load model:\n",
    "    model = createModel6()\n",
    "    model.load_weights(checkpoint_path)\n",
    "else:\n",
    "    print('wrong')\n",
    "\n",
    "checkpoint_path= \"training_5/Checkpoints_2/\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,period=1)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"]) \n",
    "result = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1000,\n",
    "        callbacks=[cp_callback],\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs = 25\n",
    "initial_lrate = 0.01\n",
    "\n",
    "def decay(epoch, steps=100):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.96\n",
    "    epochs_drop = 8\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\n",
    "\n",
    "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet.DenseNet(classes=65, input_shape=(224,224,3), depth=19, growth_rate=12,bottleneck=True, reduction=0.5)\n",
    "model.load_weights('training_4/Checkpoints/cp-0010.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('newModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.9698621852025107\n",
      "Test accuracy: 0.27411728183425316\n"
     ]
    }
   ],
   "source": [
    "#score = model.evaluate(train_generator, validation_data)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])\n",
    "score= model.evaluate_generator(validation_generator, max_queue_size=10)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-04ae2a7ad703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "model.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted= model.predict_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted[0]\n",
    "#y_pred = np.rint(predicted)\n",
    "#y_true = validation_generator.classes\n",
    "#print (classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          Bhindi       0.37      0.48      0.42        54\n",
      "         Briyani       0.30      0.22      0.25        97\n",
      "            Naan       0.27      0.60      0.38       140\n",
      "           Halwa       0.23      0.82      0.36       523\n",
      "          Imarti       0.44      0.06      0.11        62\n",
      "   Chole bhatura       0.29      0.22      0.25       171\n",
      "     Daal makhni       0.00      0.00      0.00        75\n",
      "          jalebi       0.00      0.00      0.00        84\n",
      "         Cachuri       0.56      0.08      0.14        63\n",
      "            Kadi       0.41      0.17      0.24       215\n",
      "           Barfi       0.75      0.04      0.07        82\n",
      "           kheer       0.36      0.53      0.43       172\n",
      "           kulfi       0.33      0.05      0.09       113\n",
      "           ladoo       0.00      0.00      0.00        69\n",
      "      Pav bhaaji       0.14      0.03      0.05        61\n",
      "           Momos       0.31      0.06      0.10        65\n",
      "           Papad       0.29      0.12      0.17        78\n",
      "       Ras malai       0.00      0.00      0.00        67\n",
      "            Roti       0.00      0.00      0.00        69\n",
      "            Upma       0.15      0.18      0.16        72\n",
      "       Aloo_gobi       0.00      0.00      0.00        50\n",
      "      Aloo_matar       0.33      0.13      0.18        79\n",
      "      Aloo_methi       0.54      0.61      0.57       196\n",
      "           Bajji       0.27      0.57      0.36        44\n",
      "         Bhatura       0.00      0.00      0.00        58\n",
      "           Bonda       0.39      0.26      0.31        74\n",
      "  Butter_Chicken       0.00      0.00      0.00        48\n",
      "           Chaap       0.00      0.00      0.00        67\n",
      "    Chana_masala       0.27      0.37      0.31        83\n",
      "      chicken_65       0.00      0.00      0.00        70\n",
      "    chole_kulche       0.00      0.00      0.00        65\n",
      "     Dahi_bhalla       0.13      0.10      0.12        69\n",
      "       Dahi_Vada       0.18      0.17      0.17        78\n",
      "             dal       0.26      0.54      0.35       112\n",
      "          Dhokla       0.21      0.13      0.16        82\n",
      "            Dosa       0.00      0.00      0.00        53\n",
      "           iddli       0.25      0.44      0.32        73\n",
      "    Kadai_paneer       0.00      0.00      0.00        78\n",
      "           Kabab       0.33      0.02      0.03        62\n",
      "         Khandvi       0.07      0.05      0.05        66\n",
      "    litti_chokha       0.33      0.64      0.43        90\n",
      "          Maggie       0.00      0.00      0.00        72\n",
      "     Malai_kofta       0.26      0.06      0.09        89\n",
      "    Matar_paneer       0.23      0.43      0.30        90\n",
      "           Modak       0.56      0.24      0.34       115\n",
      "    Palak_Paneer       0.14      0.07      0.09        90\n",
      "        Panipuri       0.16      0.04      0.07        90\n",
      "         Paratha       0.13      0.37      0.19       108\n",
      "            Poha       0.00      0.00      0.00        90\n",
      "           Puttu       0.20      0.11      0.14        90\n",
      "     RajmaChawal       0.17      0.01      0.02        90\n",
      "           Rajma       0.28      0.10      0.15        90\n",
      "      Rasam_Rice       0.00      0.00      0.00        85\n",
      "            Rice       0.00      0.00      0.00        76\n",
      "       RoganJosh       0.14      0.04      0.07        90\n",
      "          Sambar       0.24      0.21      0.23        70\n",
      "          Samosa       0.16      0.19      0.17        90\n",
      "    Shahi_paneer       0.12      0.16      0.13        90\n",
      "       Shrikhand       0.29      0.06      0.09        90\n",
      "      Springroll       0.55      0.13      0.21        90\n",
      "Tandoori_Chicken       0.15      0.05      0.07        81\n",
      "          Thepla       0.67      0.02      0.04        90\n",
      "         Uttapam       0.37      0.50      0.42       115\n",
      "        Vada_pav       0.37      0.92      0.53       183\n",
      "            Vada       0.36      0.54      0.43       251\n",
      "\n",
      "       micro avg       0.27      0.27      0.27      6344\n",
      "       macro avg       0.22      0.18      0.16      6344\n",
      "    weighted avg       0.25      0.27      0.21      6344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "from keras.utils import np_utils\n",
    "y_curr = np.zeros(6344)\n",
    "for i in range(0,6344):\n",
    "    index = np.argmax(predicted[i])\n",
    "    y_curr[i] = index\n",
    "\n",
    "y_test= validation_generator.classes\n",
    "\n",
    "cc = ['Bhindi','Briyani','Naan','Halwa', 'Imarti', 'Chole bhatura', 'Daal makhni', 'jalebi', 'Cachuri', 'Kadi', 'Barfi', 'kheer','kulfi', 'ladoo', 'Pav bhaaji', 'Momos', 'Papad', 'Ras malai', 'Roti', 'Upma', 'Aloo_gobi', 'Aloo_matar', 'Aloo_methi', 'Bajji', 'Bhatura', 'Bonda', 'Butter_Chicken', 'Chaap', 'Chana_masala', 'chicken_65', 'chole_kulche', 'Dahi_bhalla', 'Dahi_Vada', 'dal', 'Dhokla', 'Dosa', 'iddli', 'Kadai_paneer', 'Kabab', 'Khandvi', 'litti_chokha', 'Maggie', 'Malai_kofta', 'Matar_paneer', 'Modak', 'Palak_Paneer', 'Panipuri', 'Paratha', 'Poha', 'Puttu', 'RajmaChawal', 'Rajma', 'Rasam_Rice', 'Rice', 'RoganJosh', 'Sambar', 'Samosa', 'Shahi_paneer', 'Shrikhand', 'Springroll', 'Tandoori_Chicken', 'Thepla', 'Uttapam', 'Vada_pav', 'Vada' ]                                           \n",
    "print(classification_report(y_test, y_curr,target_names=cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bbcbfca725c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/AdvanceCV/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AdvanceCV/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AdvanceCV/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AdvanceCV/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AdvanceCV/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_curr = np.zeros(y_pred.shape)\n",
    "for i in range(0,len(y_pred)):\n",
    "    index = np.argmax(y_pred[i])\n",
    "    y_curr[i][index] = 1\n",
    "\n",
    "target_names = ['Bhindi','Briyani','Naan','Halwa', 'Imarti', 'Chole bhatura', 'Daal makhni', 'jalebi', 'Cachuri', 'Kadi', 'Barfi', 'kheer','kulfi', 'ladoo', 'Pav bhaaji', 'Momos', 'Papad', 'Ras malai', 'Roti', 'Upma', 'Aloo_gobi', 'Aloo_matar', 'Aloo_methi', 'Bajji', 'Bhatura', 'Bonda', 'Butter_Chicken', 'Chaap', 'Chana_masala', 'chicken_65', 'chole_kulche', 'Dahi_bhalla', 'Dahi_Vada', 'dal', 'Dhokla', 'Dosa', 'iddli', 'Kadai_paneer', 'Kabab', 'Khandvi', 'litti_chokha', 'Maggie', 'Malai_kofta', 'Matar_paneer', 'Modak', 'Palak_Paneer', 'Panipuri', 'Paratha', 'Poha', 'Puttu', 'RajmaChawal', 'Rajma', 'Rasam_Rice', 'Rice', 'RoganJosh', 'Sambar', 'Samosa', 'Shahi_paneer', 'Shrikhand', 'Springroll', 'Tandoori_Chicken', 'Thepla', 'Uttapam', 'Vada_pav', 'Vada' ]                                           \n",
    "print(classification_report(y_test, y_curr,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-545d2d336bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print('Test loss:', score[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print('Test accuracy:', score[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "model = load_model('my_modelF.h5')\n",
    "#score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "print(model.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 1920 variables.\n",
      "INFO:tensorflow:Converted 1920 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training/my_model.pb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('saveModel.h5')\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "    \n",
    "    \n",
    "frozen_graph = freeze_session(K.get_session(),output_names=[out.op.name for out in model.outputs])\n",
    "tf.train.write_graph(frozen_graph, \"./training\", \"my_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_modelFF.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Test data\n",
    "def createDataSplit():\n",
    "    input_folder = './DataSet'\n",
    "    \n",
    "    percent = 0.7\n",
    "    ListFolder = os.listdir('./DataSet')\n",
    "\n",
    "    for classes in ListFolder:\n",
    "        print(classes,' ',end = '')\n",
    "        imagePath = input_folder+'/'+classes\n",
    "        class_data = []\n",
    "        class_data_name = []\n",
    "        try:\n",
    "            a = os.listdir(imagePath)\n",
    "        except:\n",
    "            print('Wrong')\n",
    "            continue\n",
    "        for images in os.listdir(imagePath):\n",
    "            try:\n",
    "                im = Image.open(imagePath+'/'+images)\n",
    "                imgMat = np.array(im)\n",
    "                if imgMat.shape[2] == 3: \n",
    "                    class_data.append(imgMat)\n",
    "                    class_data_name.append(imagePath+'/'+images)\n",
    "                else:\n",
    "                    print(imgMat.shape)\n",
    "            except:\n",
    "                print('LoL')\n",
    "    #     print(len(class_data))\n",
    "        sizeofclass = len(class_data)\n",
    "    \n",
    "        num_of_Train_exp = math.floor(sizeofclass*(percent))\n",
    "\n",
    "        X_train = class_data[0:num_of_Train_exp]\n",
    "        X_test = class_data[num_of_Train_exp:]\n",
    "\n",
    "        TrainSet = class_data_name[0 : num_of_Train_exp]\n",
    "        ValidationSet = class_data_name[num_of_Train_exp : ]\n",
    "\n",
    "        print(sizeofclass,len(TrainSet),len(ValidationSet),end ='')\n",
    "        \n",
    "\n",
    "        directory = './DATASET/Training/'+classes\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        i = 1\n",
    "        for imagesPath in TrainSet:\n",
    "            dest2 = directory +'/'+str(i)+'.jpg'\n",
    "            copyfile(imagesPath, dest2)\n",
    "            #print(dest2,imagesPath)\n",
    "            i = i + 1\n",
    "            \n",
    "\n",
    "        directory = './DATASET/Validation/'+classes\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        i = 1\n",
    "        for imagesPath in ValidationSet:\n",
    "            dest2 = directory +'/'+str(i)+'.jpg'\n",
    "            copyfile(imagesPath, dest2)\n",
    "            #print(dest2,imagesPath)\n",
    "            i = i + 1\n",
    "            \n",
    "#createDataSplit()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_TEST_DATA = []\n",
    "# Y_TEST_DATA = []\n",
    "\n",
    "# input_folder_test = './ResizedImg/test'\n",
    "# for classes in os.listdir(input_folder):\n",
    "#     imagePath = input_folder+'/'+classes\n",
    "#     for images in os.listdir(imagePath):\n",
    "#         im = Image.open(imagePath+'/'+images)\n",
    "#         imgMat = np.array(im)\n",
    "#         X_TEST_DATA.append(imgMat)\n",
    "#         Y_TEST_DATA.append(classes)\n",
    "\n",
    "# X_TRAIN_DATA = []\n",
    "# Y_TRAIN_DATA = []\n",
    "\n",
    "# input_folder_test = './ResizedImg/train'\n",
    "# for classes in os.listdir(input_folder):\n",
    "#     imagePath = input_folder+'/'+classes\n",
    "#     for images in os.listdir(imagePath):\n",
    "#         im = Image.open(imagePath+'/'+images)\n",
    "#         imgMat = np.array(im)\n",
    "#         X_TRAIN_DATA.append(imgMat)\n",
    "#         Y_TRAIN_DATA.append(classes)\n",
    "\n",
    "# x_train_F = [item for sublist in x_train for item in sublist]\n",
    "# y_train_F = [item for sublist in y_train for item in sublist]\n",
    "# x_test_F  = [item for sublist in x_test for item in sublist]\n",
    "# y_test_F  = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "# x_train_w = []\n",
    "# y_train_w = []\n",
    "# x_test_w = []\n",
    "# y_test_w = []\n",
    "\n",
    "# for img in x_train:\n",
    "#     try:\n",
    "#         if img.shape == (224,224,3):\n",
    "#             x_train_w.append(np.array(img))\n",
    "#         else:\n",
    "#             print('wrong',img.shape)\n",
    "#     except:\n",
    "#         print('LOL')\n",
    "\n",
    "# for img in y_train:\n",
    "#     try:\n",
    "#         if img.shape == (224,224,3):\n",
    "#             y_train_w.append(np.array(img))\n",
    "#         else:\n",
    "#             print('wrong',img.shape)\n",
    "#     except:\n",
    "#         print('LOL')\n",
    "\n",
    "# for img in x_test:\n",
    "#     try:\n",
    "#         if img.shape == (224,224,3):\n",
    "#             x_test_w.append(np.array(img))\n",
    "#         else:\n",
    "#             print('wrong',img.shape)\n",
    "#     except:\n",
    "#         print('LOL')\n",
    "\n",
    "# for img in y_test:\n",
    "#     try:\n",
    "#         if img.shape == (224,224,3):\n",
    "#             y_test_w.append(np.array(img))\n",
    "#         else:\n",
    "#             print('wrong',img.shape)\n",
    "#     except:\n",
    "#         print('LOL')\n",
    "\n",
    "# x_train = np.array(x_train_w, dtype=np.float32)\n",
    "# y_train = np.array(y_train_w, dtype=np.float32)\n",
    "# x_test  = np.array(x_test_w, dtype=np.float32)\n",
    "# y_test  = np.array(y_test_w, dtype=np.float32)\n",
    "\n",
    "# img = Image.fromarray(np.array(X_train[0]))\n",
    "# img\n",
    "\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# print(x_train[0])\n",
    "# print(len(x_train[0]))\n",
    "# im = Image.open(x_train[0])\n",
    "# im.show()\n",
    "# print('before changing')\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "#     print(\"hola\")\n",
    "# else:\n",
    "#     print(\"hola8\")\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# print(x_train[0])\n",
    "# print(len(x_train[0][0][0]))\n",
    "# print(x_train[0].shape)\n",
    "# print(np.zeros((3,3,1)))\n",
    "\n",
    "# im1 = Image.open(\"b.jpeg\").convert('LA')\n",
    "# im1 = Image.open(\"b.jpeg\")\n",
    "# im2=ImageOps.grayscale(im1)\n",
    "# Arr1 = np.array(im1)\n",
    "# Arr2 = np.array(im2)\n",
    "# print('shape of image is',Arr.shape,' value are ',Arr[0][0])\n",
    "# Arr.show()\n",
    "# img = Image.fromarray(Arr)\n",
    "# img.show()    \n",
    "\n",
    "# print(x_train[0].type)\n",
    "# '''\n",
    "# import tensorflow as tf \n",
    "# sess = tf.Session()\n",
    "# K.set_session(sess)\n",
    "\n",
    "# class TFCheckpointCallback(keras.callbacks.Callback):\n",
    "#     def __init__(self,saver,sess):\n",
    "#         self.saver=saver\n",
    "#         self.sess = sess\n",
    "#     def on_epoch_end(self,epoch,logs=None):\n",
    "#         self.saver.save(self.sess,'freeze/checkpoint',global_step=epoch)\n",
    "\n",
    "# tf_saver = tf.train.Saver(max_to_keep=2)\n",
    "# checkpoint_callback = TFCheckpointCallback(tf_saver,sess)\n",
    "# sess.close()\n",
    "# '''\n",
    "\n",
    "\n",
    "# def prepare_graph_for_freezing(model_folder):\n",
    "#     model = createModel6()\n",
    "#     cp = tf.train.get_checkpoint_state(model_folder,latest_filename='cp-0020.ckpt')\n",
    "#     input_cp = cp.model_checkpoint_path\n",
    "#     saver = tf.train.Saver()\n",
    "#     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "#         K.set_Session(sess)\n",
    "#         saver.restore(sess,input_cp)\n",
    "#         tf.gfile.MakeDirs(model_folder+'freeze')\n",
    "#         saver.save(sess,model_folder+'freeze/checkpoint',global_step=0)\n",
    "\n",
    "# def freeze_graph(model_folder):\n",
    "#     cp = tf.train.get_checkpoint_state(model_folder,latest_filename='cp-0019.ckpt')\n",
    "#     input_cp = cp.model_checkpoint_path\n",
    "#     absolute_model_folder = '/'.join(input_cp.split('/')[:-1])\n",
    "#     output_graph = absolute_model_folder+'/frozen_model.pb'\n",
    "#     output_node_name='output/Softmax'\n",
    "#     clear_devices=True\n",
    "#     new_saver = tf.train.import_meta_graph(input_cp+'.meta',clear_devices=clear_devices)\n",
    "#     graph=tf.get_default_graph()\n",
    "#     input_graph_def = graph.as_graph_def()\n",
    "    \n",
    "#     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess2:\n",
    "#         new_saver.restore(sess2,input_cp)\n",
    "#         output_graph_def = graph_util.convert_variables_to_constants(sess2,input_graph_def,output_node_name.split(','))\n",
    "#         with tf.gfile.GFile(output_graph,'wb') as f:\n",
    "#             f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "# #tf.reset_default_graph()\n",
    "# #prepare_graph_for_freezing('training/Checkpoints/')\n",
    "# freeze_graph('training/Checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(x_train))\n",
    "# print(len(y_train))\n",
    "\n",
    "# print(len(x_test))\n",
    "# print(len(y_test))\n",
    "\n",
    "# a = list(zip(x_train,y_train))\n",
    "# random.shuffle(a)\n",
    "# x_train , y_train = zip(*a)\n",
    "\n",
    "# a = list(zip(x_test,y_test))\n",
    "# random.shuffle(a)\n",
    "# x_test , y_test = zip(*a)\n",
    "# del a\n",
    "\n",
    "# x_train = np.array(list(x_train), dtype=np.float16)\n",
    "# y_train = np.array(list(y_train), dtype=np.float16)\n",
    "# x_test  = np.array(list(x_test), dtype=np.float16)\n",
    "# y_test  = np.array(list(y_test), dtype=np.float16)\n",
    "# y_train = keras.utils.np_utils.to_categorical(y_train, 20)\n",
    "# y_test = keras.utils.np_utils.to_categorical(y_test, 20)\n",
    "\n",
    "# # Normalization\n",
    "\n",
    "# x_train /= 255;\n",
    "# x_test /= 255;\n",
    "\n",
    "# # Saving train and test set in a file\n",
    "\n",
    "# np.save('x_train.npy',x_train)\n",
    "# np.save('y_train.npy',y_train)\n",
    "# np.save('x_test.npy',x_test)\n",
    "# np.save('y_test.npy',y_test)\n",
    "\n",
    "# # Loading Train and Test Set\n",
    "\n",
    "# x_train = np.load('x_train.npy')\n",
    "# y_train = np.load('y_train.npy')\n",
    "# x_test  = np.load('x_test.npy')\n",
    "# y_test  = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel77():\n",
    "    #base_model= keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "    model = Sequential()\n",
    "    #256\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3),strides=2, activation='relu',input_shape=(224,224,3),name='layer1'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer1_norm'))\n",
    "    model.add(Activation('relu',name='layer1_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3),strides=1, activation='relu',name='layer2'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer2_norm'))\n",
    "    model.add(Activation('relu',name='layer2_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3),strides=1, activation='relu',name='layer3'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer3_norm'))\n",
    "    model.add(Activation('relu',name='layer3_act'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=2,name='layer3_pool'))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3),strides=1, activation='relu',name='layer4'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer4_norm'))\n",
    "    model.add(Activation('relu',name='layer4_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1, activation='relu',name='layer5'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer5_norm'))\n",
    "    model.add(Activation('relu',name='layer5_act'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=2,name='layer5_pool'))\n",
    "    \n",
    "    #128\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1, activation='relu',name='layer6', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer6_norm'))\n",
    "    model.add(Activation('relu',name='layer6_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1, activation='relu',name='layer7', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer7_norm'))\n",
    "    model.add(Activation('relu',name='layer7_act'))\n",
    "    #64    \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1, activation='relu',name='layer8', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer8_norm'))\n",
    "    model.add(Activation('relu',name='layer8_act'))\n",
    "    #32\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),strides=1,padding='same', activation='relu',name='layer9'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=1,padding='same', data_format=None, name='layer9_Apool'))\n",
    "    #128\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),strides=1, activation='relu',name='layer10', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer10_norm'))\n",
    "    model.add(Activation('relu',name='layer10_act'))\n",
    "    #128\n",
    "    model.add(Conv2D(filters=1024, kernel_size=(3,3),strides=1, activation='relu',name='layer11', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer11_norm'))\n",
    "    model.add(Activation('relu',name='layer11_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=1024, kernel_size=(3,3),strides=1, activation='relu',name='layer12', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer12_norm'))\n",
    "    model.add(Activation('relu',name='layer12_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=1, kernel_size=(3,3),strides=1, activation='relu',name='layer13', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer13_norm'))\n",
    "    model.add(Activation('relu',name='layer13_act'))\n",
    "    \n",
    "   # model.add(Flatten())\n",
    "   # model.add(Dense(2000,activation='relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(48,activation='softmax'))\n",
    "    \n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "#createModel7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "# img_rows, img_cols ,channel = 224, 224 , 3\n",
    "def createModel1():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3),strides=1,padding='same',activation='relu',input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(filters=20, kernel_size=(3,3),strides=1,activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=9, kernel_size=(1,1),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(5,5), strides=5, padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1089,activation='relu'))\n",
    "    model.add(Dense(400,activation='relu'))\n",
    "    model.add(Dense(20,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel2():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    x = base_model.output\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(400, activation='relu')(x)\n",
    "    \n",
    "    predictions = Dense(20, activation='softmax')(x)\n",
    "\n",
    "    m = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    for layer in m.layers[:249]:\n",
    "        layer.trainable = False\n",
    "    for layer in m.layers[249:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel3():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(400, activation='relu')(x)\n",
    "    # and a logistic layer -- \n",
    "    predictions = Dense(20, activation='softmax')(x)\n",
    "\n",
    "    m = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    for layer in m.layers[:249]:\n",
    "        layer.trainable = False\n",
    "    for layer in m.layers[249:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols ,channel = 224, 224 , 3\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "def createModel4():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3),strides=1,padding='same',activation='relu',input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(filters=1096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=2096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=2096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=3096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=3096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=4096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=5012, kernel_size=(1,1),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=5012, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=6012, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=7017, kernel_size=(1,1),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    \n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=1096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=1096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=2096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=3096, kernel_size=(1,1),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=3096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=4096, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    #model.add(Conv2D(filters=128, kernel_size=(1,1),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(5096,activation='relu'))\n",
    "    model.add(Dense(2000,activation='relu'))\n",
    "    model.add(Dense(20,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols ,channel = 224, 224 , 3\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "def createMode10():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3),strides=1,padding='same',activation='relu',input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(1,1),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1,padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(1,1),strides=1,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dense(2000,activation='relu'))\n",
    "    model.add(Dense(20,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel5():\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet',input_shape=(224,224,3))\n",
    "\n",
    "    # # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(400, activation='relu')(x)\n",
    "    # and a logistic layer -- \n",
    "    predictions = Dense(101, activation='softmax')(x)\n",
    "\n",
    "    m = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    for layer in m.layers[-3:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel6():\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet',input_shape=(224,224,3))\n",
    "    #base_model= keras.applications.densenet.DenseNet121(include_top=False,input_shape=(224,224,3))\n",
    "        \n",
    "    x = base_model.output\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    # and a logistic layer -- \n",
    "    predictions = Dense(48, activation='softmax',name='output')(x)\n",
    "\n",
    "    m = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    for layer in m.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "#    for layer in m.layers[-2:]:\n",
    "#        layer.trainable = True\n",
    "\n",
    "    #m.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel7():\n",
    "    #base_model= keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3),strides=2, activation='relu',input_shape=(224,224,3),name='layer1'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer1_norm'))\n",
    "    model.add(Activation('relu',name='layer1_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3),strides=1, activation='relu',name='layer2'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer2_norm'))\n",
    "    model.add(Activation('relu',name='layer2_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1, activation='relu',name='layer3'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer3_norm'))\n",
    "    model.add(Activation('relu',name='layer3_act'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=2,name='layer3_pool'))\n",
    "    \n",
    "    model.add(Conv2D(filters=80, kernel_size=(3,3),strides=1, activation='relu',name='layer4'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer4_norm'))\n",
    "    model.add(Activation('relu',name='layer4_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=192, kernel_size=(3,3),strides=1, activation='relu',name='layer5'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer5_norm'))\n",
    "    model.add(Activation('relu',name='layer5_act'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=2,name='layer5_pool'))\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1, activation='relu',name='layer6', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer6_norm'))\n",
    "    model.add(Activation('relu',name='layer6_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=48, kernel_size=(3,3),strides=1, activation='relu',name='layer7', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer7_norm'))\n",
    "    model.add(Activation('relu',name='layer7_act'))\n",
    "        \n",
    "    model.add(Conv2D(filters=96, kernel_size=(3,3),strides=1, activation='relu',name='layer8', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer8_norm'))\n",
    "    model.add(Activation('relu',name='layer8_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=192, kernel_size=(3,3),strides=1,padding='same', activation='relu',name='layer9'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=1,padding='same', data_format=None, name='layer9_Apool'))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1, activation='relu',name='layer10', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer10_norm'))\n",
    "    model.add(Activation('relu',name='layer10_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),strides=1, activation='relu',name='layer11', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer11_norm'))\n",
    "    model.add(Activation('relu',name='layer11_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=96, kernel_size=(3,3),strides=1, activation='relu',name='layer12', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer12_norm'))\n",
    "    model.add(Activation('relu',name='layer12_act'))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3),strides=1, activation='relu',name='layer13', padding='same'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', name='Layer13_norm'))\n",
    "    model.add(Activation('relu',name='layer13_act'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2000,activation='relu'))\n",
    "\n",
    "    model.add(Dense(48,activation='softmax'))\n",
    "    \n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "#createModel7()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
